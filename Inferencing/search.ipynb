{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "0e0e7f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from typing import Any\n",
    "from dataclasses import dataclass,field\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "import os\n",
    "import joblib\n",
    "from autocorrect import Speller\n",
    "import regex as re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import util\n",
    "import torch\n",
    "import time\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "a8328c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data class for response object of search endpoint\n",
    "@dataclass\n",
    "class SearchResponse:\n",
    "    \n",
    "    data: list = field(default_factory=list)\n",
    "    query_id: str = \"\" \n",
    "    response: list = field(default_factory=list)\n",
    "    error_code: int = 0\n",
    "    error_msg: str = \"Success\"\n",
    "\n",
    "    def search_response(self) -> Any:\n",
    "        return json.dumps( \n",
    "            { \n",
    "                \"fdk_response\": { \n",
    "                    \"query_id\": self.query_id, \n",
    "                    \"query\": self.data, \n",
    "                    \"result\": self.response\n",
    "                }, \n",
    "                \"error\": {\n",
    "                    \"error_code\": self.error_code, \n",
    "                    \"error_message\": self.error_msg\n",
    "                }\n",
    "            }\n",
    "    )\n",
    "\n",
    "# This is the dataclass for the recommendations generated by the algorithm\n",
    "@dataclass\n",
    "class PredictionData:\n",
    "    query_id: Any\n",
    "    query: Any\n",
    "    response: Any\n",
    "    feedback: str = \"\"\n",
    "    rating: str = \"\"\n",
    "    timestamp: datetime = datetime.now().strftime('%Y%m%d-%H:%M:%S')\n",
    "\n",
    "    def record_dictionary(self):\n",
    "        return { \"Query_ID\": [self.query_id],\n",
    "        \"Timestamp\": [self.timestamp],\n",
    "        \"Query\": [self.query],\n",
    "        \"Response\": [self.response],\n",
    "        \"Feedback\": [self.feedback],\n",
    "        \"Rating\": [self.rating],\n",
    "        }\n",
    "\n",
    "        # return \"\\n\"+str(self.query_id)+\",\"+str(self.timestamp)+\",\"+str(self.query)+\",\"+str(self.response)+\",\"+str(self.feedback)+\",\"+str(self.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "732ea521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_prediction_files(prediction_folder):\n",
    "    feedbacks = []\n",
    "    paths = os.listdir(prediction_folder)\n",
    "    for filename in paths:\n",
    "        print(\"Reading file: \", filename)\n",
    "        feedbacks.append(pd.read_csv(prediction_folder+filename))\n",
    "        print(feedbacks)\n",
    "    predicted_data = pd.concat(feedbacks)[['Query','Feedback']].dropna().reset_index(drop=True)\n",
    "    #featurisation on predicted data\n",
    "    remove_words = \"feedback|result|response|computer|not|resolved|test\"\n",
    "    predicted_data_copy = predicted_data.copy()\n",
    "    for ind,feedback in zip(predicted_data_copy.index,predicted_data_copy[\"Feedback\"]):\n",
    "        if re.findall(remove_words, feedback.lower()) or len(feedback.split(\" \")) < 3:\n",
    "            predicted_data = predicted_data.drop(ind)\n",
    "    for ind in predicted_data.index:\n",
    "        predicted_data['Query'][ind] = predicted_data['Query'][ind][2:-2]\n",
    "    return predicted_data\n",
    "\n",
    "def result_prediction_file(model,feedback_data,query):\n",
    "    # feedback query searching\n",
    "    results1 = []\n",
    "    for f in feedback_data[\"Query\"]:\n",
    "        print(repr(f))\n",
    "    top_k = 5\n",
    "    if top_k > len(feedback_data):\n",
    "        top_k = len(feedback_data)\n",
    "\n",
    "    #Sentences are encoded by calling model.encode()\n",
    "    query_embeddings = model.encode(query,convert_to_tensor=True)\n",
    "    embeddings = model.encode(list(feedback_data[\"Query\"]),convert_to_tensor=True)\n",
    "\n",
    "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
    "    cos_scores = util.cos_sim(query_embeddings, embeddings)[0]\n",
    "    print(cos_scores)\n",
    "    top_results = torch.topk(cos_scores, k=top_k)\n",
    "\n",
    "    for score, idx in zip(top_results[0], top_results[1]):\n",
    "        results1.append([list(feedback_data[\"Query\"])[idx],list(feedback_data[\"Feedback\"])[idx],round(score.item(),2)])\n",
    "\n",
    "    # final results\n",
    "    feedback_results = pd.DataFrame(columns=[\"Response\",\"Scores\"])\n",
    "    for res in results1:\n",
    "        if res[2] >= 0.95:\n",
    "            feedback_results.loc[len(feedback_results.index)] = [res[1], res[2]]\n",
    "        else:\n",
    "            break\n",
    "    return feedback_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "53693042",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TenantSearchClass:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Loading Model, Embedded corpus & Issues data into global variable\n",
    "        Model -->> Symmetric and Asymetric Semantic search model\n",
    "        Issue Data -->> Preprocessed Dataset\n",
    "        Embedded Corpus -->> Encoded short description and cause object data for runtime comparision\n",
    "        \"\"\"\n",
    "        self.top_k: int\n",
    "        self.short_description_embedded_corpus: Any\n",
    "        self.cause_object_embedded_corpus: Any\n",
    "        self.issue_data: Any\n",
    "        self.symmetric_model: Any\n",
    "        self.asymmetric_model: Any\n",
    "        self.CDL_DIRECTORY_FILE = \"/Prediction/US3/prediction_\"\n",
    "        self.CDL_PREDICTION_FOLDER = \"/Prediction/US3\"\n",
    "        self.spell: Any\n",
    "\n",
    "        model_path=\"../ModelTraining\"\n",
    "\n",
    "        # Loading Issue data\n",
    "        issue_data_path = \"../Preprocessed/processed_dataset.csv\"\n",
    "        self.issue_data = pd.read_csv(issue_data_path)\n",
    "\n",
    "        # Loading Symmetric Semantic Search Model\n",
    "        symmetric_model_path = model_path + \"/symmetric_model.pkl\"\n",
    "        self.symmetric_model = joblib.load(symmetric_model_path)\n",
    "\n",
    "        # Loading Asymmetric Semantic Search Model\n",
    "        # Quickfix validation for Gen2 migration \n",
    "        # asymmetric_model_path = model_path + \"/\" + str(cfg[\"asymmetric_search_model_name\"])\n",
    "        # self.asymmetric_model = joblib.load(asymmetric_model_path)\n",
    "        self.asymmetric_model = SentenceTransformer('sentence-transformers/msmarco-distilbert-base-v4')\n",
    "\n",
    "        # Loading Short Descripiton Embedded corpus\n",
    "        short_description_corpus_path = model_path + \"/short_description_corpus_data.pkl\"\n",
    "        self.short_description_embedded_corpus = joblib.load(short_description_corpus_path)\n",
    "\n",
    "        # Loading Cause Object Embedded corpus\n",
    "        cause_object_corpus_path = model_path + \"/cause_object_corpus_data.pkl\"\n",
    "        self.cause_object_embedded_corpus = joblib.load(cause_object_corpus_path)\n",
    "\n",
    "        # Spelling correction\n",
    "        self.spell = Speller(lang='en')\n",
    "\n",
    "    def data_processing(self,data) -> Any:\n",
    "        # Tokenize the query\n",
    "        process_data = data\n",
    "        tokens = [i for item in data for i in item.split()]\n",
    "\n",
    "        # Joining the tokens\n",
    "        context_dictionary = ['ls12','ls10','ls18','ls24','rtd','d11','d13','d14','v24','limit','ls9','v2','v7','chariot',\n",
    "                            'ls15','v22','ls2','membrane','locking','rotation','d10','open','d17','precon','v6','ls8,',\n",
    "                            'ls21','press','lock','opening','loading','d1bis','ls5','steam','ls27','v3','ls20','vinj',\n",
    "                            'd02','d19','d06','d18','lid','raise','d09','valves','ls28','cylinder','d07','ls6','vter',\n",
    "                            'v1','v4bis','d03','v9','hooks','release','unloading','ls16','v1bis','v4ebis','to','ls17',\n",
    "                            'd19bis','close','jib','d04','hydraulic','ls1','d1ter','d20','ring','v23','sectors','d15',\n",
    "                            'preconfirmation','hook','d01','closing','unlock','head','v34','ls7,','pin','ls19','lower',\n",
    "                            'v19','d20bis','d12','fingers','d05','d3bis','switches','moldback', 'orifice', 'valve',\n",
    "                            'kpot','reheat','dilate']\n",
    "        p = re.compile(r\"\\L<words>\", words=context_dictionary)\n",
    "        for idx, val in enumerate(tokens):\n",
    "            if not p.search(val.lower()):\n",
    "                tokens[idx] = self.spell(tokens[idx].lower())\n",
    "            process_data = [' '.join(tokens)]\n",
    "\n",
    "        return process_data\n",
    "\n",
    "    def symmetric_semantic_search(self,feature_name,query):\n",
    "\n",
    "        # assigning dataframe,long description and feature into variables\n",
    "        df_updated,solution = self.issue_data,\"WO rem long desc\"\n",
    "        cfg = {\"cause_object\": \"Cause object\",\n",
    "            \"short_description\": \"WO Description\",\n",
    "            \"long_description\": \"WO rem long desc\"}\n",
    "        feature = cfg[feature_name]\n",
    "\n",
    "        # creating corpus of unique feature values\n",
    "        corpus = list(df_updated[feature].drop_duplicates())\n",
    "\n",
    "        # defining top k with constant value\n",
    "        top_k = 5\n",
    "\n",
    "        # encoding query\n",
    "        query_embeddings = self.symmetric_model.encode(query,convert_to_tensor=True)\n",
    "\n",
    "        # Assigning embeddings and topk according to the feature\n",
    "        if feature == \"Cause object\":\n",
    "            embeddings = self.cause_object_embedded_corpus\n",
    "            top_k = 2\n",
    "        elif feature == \"WO Description\":\n",
    "            embeddings = self.short_description_embedded_corpus\n",
    "            top_k = 15\n",
    "\n",
    "        # if corpus length is less that topk then changing its value\n",
    "        if top_k > len(corpus):\n",
    "            top_k = len(corpus)\n",
    "\n",
    "        # calculation cosine scores using cosine similarity and\n",
    "        # generating top results using torch.topk\n",
    "        # between query and feature embeddings\n",
    "        cos_scores = util.cos_sim(query_embeddings, embeddings)[0]\n",
    "        top_results = torch.topk(cos_scores, k=top_k)\n",
    "\n",
    "        #saving dataset rows which has top k feature in the dataframe\n",
    "        filtered_feature = []\n",
    "        i = 0\n",
    "        for score, idx in zip(top_results[0], top_results[1]):\n",
    "            filtered_feature.append(corpus[top_results[1][i]])\n",
    "            i += 1\n",
    "        df_feature = df_updated[df_updated[feature].isin(filtered_feature)]\n",
    "\n",
    "        #nltk tokenization of the feature filtered dataframe\n",
    "        corpus = []\n",
    "        for index,row in df_feature.drop_duplicates(subset=solution,keep=\"first\").iterrows():\n",
    "            sen = row[solution]\n",
    "            token = nltk.word_tokenize(sen)\n",
    "            tagged_list = nltk.pos_tag(token)\n",
    "            corpus.append([tagged_list,row[feature]])\n",
    "\n",
    "\n",
    "        # extractong the solution which have verbs second form and\n",
    "        # making two lists for solution and feature\n",
    "        feature_corpus =[]\n",
    "        feature_list = []\n",
    "        for sen1 in corpus:\n",
    "            for t in sen1[0]:\n",
    "                if t[1] == \"VBD\" or t[1] == \"VBN\":\n",
    "                    feature_corpus.append(\" \".join([w[0] for w in sen1[0]]))\n",
    "                    feature_list.append(sen1[1])\n",
    "                    break\n",
    "        return feature_corpus,feature_list\n",
    "\n",
    "    def tenant_run(self,data,feedback_data):\n",
    "        # Invoking data preprocessing function\n",
    "        process_data = self.data_processing(data)\n",
    "\n",
    "        # initialising output list\n",
    "        outputlist = list()\n",
    "\n",
    "        # Invoking symmetric semantic search with feature cause object if the lengthof corpus is less\n",
    "        # then invoking symmetric semantic search with feature short description\n",
    "        description_corpus,feature_list = self.symmetric_semantic_search(\"cause_object\",process_data)\n",
    "\n",
    "        if len(description_corpus) < 200:\n",
    "            description_corpus,feature_list = self.symmetric_semantic_search(\"short_description\",process_data)\n",
    "\n",
    "        result2 = []\n",
    "\n",
    "        # initialising value of topk for asymmetric semantic dearch on the\n",
    "        # long description corpus which has been filtered out\n",
    "        top_k = min(15, len(description_corpus))\n",
    "\n",
    "        # encoding query and long description\n",
    "        query_embedding = self.asymmetric_model.encode(process_data,convert_to_tensor=True)\n",
    "        passage_embedding = self.asymmetric_model.encode(description_corpus,convert_to_tensor=True)\n",
    "\n",
    "        # We use cosine-similarity and torch.topk to find the highest top_k scores\n",
    "        cos_scores = util.cos_sim(query_embedding, passage_embedding)[0]\n",
    "        top_results = torch.topk(cos_scores, k=top_k)\n",
    "\n",
    "        # saving top_k results and scores in the list\n",
    "        for score, idx in zip(top_results[0], top_results[1]):\n",
    "            result2.append((description_corpus[idx],\"{:.4f}\".format(score)))\n",
    "\n",
    "        # creating the dataframe for top_k results with three columns as response score and feature\n",
    "        result_df = pd.DataFrame(result2,columns=[\"Response\",\"Scores\"])\n",
    "        result_df[\"Cause object/Short Description\"] = \"\"\n",
    "        for i,res in enumerate(result_df[\"Response\"]):\n",
    "            result_df[\"Cause object/Short Description\"][i] = (feature_list[description_corpus.index(res)])\n",
    "\n",
    "        # extracting responses from top_k results\n",
    "        response_corpus = result_df[\"Response\"]\n",
    "\n",
    "        # encoding top_k responses\n",
    "        corpus_sentences = list(response_corpus)\n",
    "        print(\"Encode the corpus. This might take a while\")\n",
    "        corpus_embeddings = self.symmetric_model.encode(corpus_sentences,show_progress_bar=True, convert_to_tensor=True)\n",
    "\n",
    "        # clustering the top_k responses using FAST Clustering\n",
    "        print(\"Start clustering\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # clustering the tiop_k responses using FAST Clustering\n",
    "        clusters = util.community_detection(corpus_embeddings, min_community_size=1, threshold=0.75)\n",
    "\n",
    "        print(\"Clustering done after {:.2f} sec\".format(time.time() - start_time))\n",
    "\n",
    "        #extracting the nearest repsonse from each cluster\n",
    "        df_final = pd.DataFrame()\n",
    "        for i, cluster in enumerate(clusters):\n",
    "            df_final = pd.concat([df_final,pd.DataFrame(result_df.iloc[cluster[0:]].sort_values(by=\"Scores\",ascending=False))],ignore_index=True)\n",
    "        #sorting the reponses according to the scores for final recommendations\n",
    "        df_final = df_final.sort_values(by=\"Scores\",ascending=False)\n",
    "\n",
    "        # prediction result\n",
    "        try:\n",
    "            feedback_results = result_prediction_file(self.symmetric_model,feedback_data,process_data)\n",
    "        except Exception as e:\n",
    "            feedback_results = []\n",
    "        # df_final results length\n",
    "        df_len = 5 - len(feedback_results)\n",
    "\n",
    "        #combining results\n",
    "        if len(feedback_results) > 0 and len(feedback_results) < 5:\n",
    "            combined_results = feedback_results.append(df_final[[\"Response\",\"Scores\"]][:df_len]).reset_index(drop=True)\n",
    "        elif len(feedback_results) == 5:\n",
    "            combined_results = feedback_results\n",
    "        else:\n",
    "            combined_results = df_final[[\"Response\",\"Scores\"]]\n",
    "\n",
    "        return process_data,combined_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "222cd48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_operation(process_data,outputlist):\n",
    "    file_name = f\"prediction_{datetime.now().strftime('%Y%m%d')}.csv\"\n",
    "    if not os.path.isfile(file_name):\n",
    "        #--> Preparing columns list\n",
    "        column_names = ['Query_ID', 'Timestamp', 'Query', 'Response', 'Feedback', 'Rating']\n",
    "        pd.DataFrame(columns = column_names).to_csv(file_name,index=False)\n",
    "        \n",
    "    query_id = f\"{uuid.uuid4()}-{datetime.now().strftime('%Y%m%d')}\"\n",
    "\n",
    "    # Preparing new prediction record\n",
    "    new_record = PredictionData(query_id = query_id,query = process_data, response = outputlist, feedback = \"\" ,rating = \"\",timestamp=datetime.now().strftime('%Y%m%d-%H:%M:%S'))\n",
    "\n",
    "    # Append the feedback to feedback data file\n",
    "    old_df = pd.read_csv(file_name)\n",
    "    new_df = pd.DataFrame(new_record.record_dictionary())\n",
    "    updated_df = pd.concat([old_df, new_df], ignore_index=True)\n",
    "    updated_df.to_csv(file_name, index=False)\n",
    "\n",
    "    return new_record.query_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "a3e0fd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "def run(raw_data):\n",
    "    # Search Response Object\n",
    "    res = SearchResponse()\n",
    "    try:\n",
    "        # initialising output list\n",
    "        outputlist = list()\n",
    "        \n",
    "        # Data extraction\n",
    "        res.data = json.loads(raw_data)['data']\n",
    "\n",
    "        # Invoking run function of tenant specific object\n",
    "        feedback_data = read_prediction_files(\"./feedback/\")\n",
    "        print(\"Feedback data:\", feedback_data)\n",
    "        tenant = TenantSearchClass()\n",
    "        process_data, combined_results = tenant.tenant_run(res.data,feedback_data)\n",
    "\n",
    "        # output list as final recommendations\n",
    "        n = 1\n",
    "        for ind in combined_results.index:\n",
    "            if n == 4:\n",
    "                break\n",
    "            description = combined_results[\"Response\"][ind]\n",
    "            match_row = {\n",
    "                        'rank': n,\n",
    "                        'description': description,\n",
    "                        'score': combined_results[\"Scores\"][ind]\n",
    "                        }\n",
    "            outputlist.append(match_row)\n",
    "            n += 1\n",
    "\n",
    "        res.response = outputlist\n",
    "\n",
    "        # Recoding the response data into CDL\n",
    "        res.query_id = file_operation(process_data, res.response)\n",
    "\n",
    "        # Logging for future troubleshooting\n",
    "        print(json.dumps({ \"input\": process_data, \"output\": res.response }))\n",
    "\n",
    "        return res.search_response()\n",
    "\n",
    "    except Exception as error:\n",
    "        # General Exception handling for error message capturing\n",
    "        res.error_code = 500\n",
    "        res.error_msg = str(error)\n",
    "        print(\"Error occurred:\", traceback.format_exc())\n",
    "        return res.search_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "3f2b673f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file:  prediction_20230109.csv\n",
      "[                                        Query_ID          Timestamp  \\\n",
      "0  1369c9d8-75a7-4dfb-ac15-fc4bd6caf05d-20230109  20230109-18:40:14   \n",
      "1  9bb93521-e1c5-4bd9-b9ad-9c6b6c4554f6-20230109  20230109-18:57:04   \n",
      "2  78bc77ed-9b19-4999-af87-4b0f57beda6e-20230109  20230109-18:57:09   \n",
      "\n",
      "                     Query                                           Response  \\\n",
      "0  ['Machine not working']  [{'rank': 1, 'description': 'screw loose issue...   \n",
      "1  ['Machine not working']  [{'rank': 1, 'description': 'screw loose issue...   \n",
      "2  ['Machine not working']  [{'rank': 1, 'description': 'screw loose issue...   \n",
      "\n",
      "              Feedback  Rating  \n",
      "0  feedback response 1     1.0  \n",
      "1                  NaN     NaN  \n",
      "2                  NaN     NaN  ]\n",
      "Feedback data: Empty DataFrame\n",
      "Columns: [Query, Feedback]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88621/2295464434.py:170: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  result_df[\"Cause object/Short Description\"][i] = (feature_list[description_corpus.index(res)])\n",
      "/tmp/ipykernel_88621/2295464434.py:170: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  result_df[\"Cause object/Short Description\"][i] = (feature_list[description_corpus.index(res)])\n",
      "/tmp/ipykernel_88621/2295464434.py:170: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  result_df[\"Cause object/Short Description\"][i] = (feature_list[description_corpus.index(res)])\n",
      "/tmp/ipykernel_88621/2295464434.py:170: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  result_df[\"Cause object/Short Description\"][i] = (feature_list[description_corpus.index(res)])\n",
      "/tmp/ipykernel_88621/2295464434.py:170: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  result_df[\"Cause object/Short Description\"][i] = (feature_list[description_corpus.index(res)])\n",
      "/tmp/ipykernel_88621/2295464434.py:170: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  result_df[\"Cause object/Short Description\"][i] = (feature_list[description_corpus.index(res)])\n",
      "/tmp/ipykernel_88621/2295464434.py:170: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  result_df[\"Cause object/Short Description\"][i] = (feature_list[description_corpus.index(res)])\n",
      "/tmp/ipykernel_88621/2295464434.py:170: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  result_df[\"Cause object/Short Description\"][i] = (feature_list[description_corpus.index(res)])\n",
      "/tmp/ipykernel_88621/2295464434.py:170: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  result_df[\"Cause object/Short Description\"][i] = (feature_list[description_corpus.index(res)])\n",
      "/tmp/ipykernel_88621/2295464434.py:170: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  result_df[\"Cause object/Short Description\"][i] = (feature_list[description_corpus.index(res)])\n",
      "/tmp/ipykernel_88621/2295464434.py:170: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  result_df[\"Cause object/Short Description\"][i] = (feature_list[description_corpus.index(res)])\n",
      "/tmp/ipykernel_88621/2295464434.py:170: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  result_df[\"Cause object/Short Description\"][i] = (feature_list[description_corpus.index(res)])\n",
      "/tmp/ipykernel_88621/2295464434.py:170: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  result_df[\"Cause object/Short Description\"][i] = (feature_list[description_corpus.index(res)])\n",
      "/tmp/ipykernel_88621/2295464434.py:170: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  result_df[\"Cause object/Short Description\"][i] = (feature_list[description_corpus.index(res)])\n",
      "/tmp/ipykernel_88621/2295464434.py:170: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  result_df[\"Cause object/Short Description\"][i] = (feature_list[description_corpus.index(res)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encode the corpus. This might take a while\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start clustering\n",
      "Clustering done after 0.00 sec\n",
      "{\"input\": [\"machine not working\"], \"output\": [{\"rank\": 1, \"description\": \"jib stop did n't want to work . worker persuaded it to work .\", \"score\": \"0.3782\"}, {\"rank\": 2, \"description\": \"operator got it to load-could not duplicate\", \"score\": \"0.3548\"}, {\"rank\": 3, \"description\": \"from last crew . press was put on reheat anticipating that the operator would inform if it would n't load for . instead it sat down for hours not opn downtime code before noticed and found a bad membrane oring and then that the rtd cards were n't quite reading right .\", \"score\": \"0.3452\"}]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"fdk_response\": {\"query_id\": \"94b46a2b-a0b4-4a5b-88c7-d8ee5144986f-20250409\", \"query\": [\"Machine not working\"], \"result\": [{\"rank\": 1, \"description\": \"jib stop did n\\'t want to work . worker persuaded it to work .\", \"score\": \"0.3782\"}, {\"rank\": 2, \"description\": \"operator got it to load-could not duplicate\", \"score\": \"0.3548\"}, {\"rank\": 3, \"description\": \"from last crew . press was put on reheat anticipating that the operator would inform if it would n\\'t load for . instead it sat down for hours not opn downtime code before noticed and found a bad membrane oring and then that the rtd cards were n\\'t quite reading right .\", \"score\": \"0.3452\"}]}, \"error\": {\"error_code\": 0, \"error_message\": \"Success\"}}'"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(json.dumps({ \n",
    "    \"data\": ['Machine not working']\n",
    "}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fdk_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
